architecture:
    input: 
        audio ~ audio: [maxlen = 1536, nbands = 24];
    output:
        label ~ flat: [10];

    audio 
      -> Reshape: [[1536, 24, 1]]
      -> Conv2D:[filters = 128, kernel_size = 9, strides = 1, padding = 'valid', activation = 'relu', name = 'conv1']
      -> PrimaryCaps_Vector:[capsule_dim = 8, channels = 32, kernel_size = [9,9],strides = [2,2], padding = 'valid', name = 'primarycap_conv2D']
      -> DigitCaps: [num_capsule = 10, capsule_dim = 16, routings = 3, name = 'digitcaps']
      -> ClassCaps:[num_capsule = 10]
      -> label;

source:
    bind = "training_data.csv";
    input:
        img ~ from "Audio"
          -> audio: [maxlen = 1536, nbands = 24]
          -> AudioDataGenerator: [feature = 'spectrogram'];
    output:
      label ~ from "Label"
          -> flat: [10]
          -> FlatDataGenerator:[];
    params:
        batch_size = 32,
        shuffle = True,
        shuffle_init = True;

train:
    compile:
        optimizer = Adam:[lr = 0.0001],
        loss = margin_loss,
        metrics = ['accuracy'];
    run:
        epochs = 2;
    dashboard: ;






