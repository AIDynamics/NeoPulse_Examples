{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeoPulse® SageMaker Algorithm Usage Demonstration\n",
    "\n",
    "Using NeoPulse® Algorithm with Amazon SageMaker APIs\n",
    "\n",
    "This sample notebook demonstrates using NeoPulse® Algorithm ARN to run training jobs and use that result for \n",
    "inference.\n",
    "\n",
    "***Pre-Requisite:*** Please subscribe to a NeoPulse® Algorithm before proceeding with this notebook.\n",
    "\n",
    "***NOTE: Before running this notebook, please read through it and fill in the IAM role and ARN for the algorithm you wish to use.***\n",
    "\n",
    "***NOTE:*** By default, this notebook uses the AWS credentials located in ${HOME}/.aws/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of this notebook\n",
    "This notebook contains all the code necessary to build a simple Sentiment model based on the IMDB sentiment analysis data set. \n",
    "\n",
    "It contains the following files:  \n",
    "\n",
    "- <b>aws_example.ipynb</b> - this file.\n",
    "- <b>src/build_csv.py</b> - function definitions to download and preprocess the IMDB data set.\n",
    "- <b>data/training/train.nml</b> - NML script for training model.\n",
    "- <b>images/workflow.jpeg</b> - Workflow image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "The typical workflow is depicted in the figure below. This illustrates the process of training a model, creating a model package, using the model package to create a model, then using the model for batch or real-time inference.\n",
    "![fig1](img/workflow.jpeg \"Fig.1 workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using the classic IMDB dataset which is pretty small. First we'll download and pre-process the data locally, then upload that training data to S3 for use with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.build_csv import download_data, write_training_data, write_inference_data\n",
    "\n",
    "download_data()\n",
    "\n",
    "write_training_data()\n",
    "\n",
    "write_inference_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the following files:\n",
    "\n",
    "- <b>data/training/training_data.csv</b> - Training dataset. 50000 records.\n",
    "\n",
    "\n",
    "- <b>data/transform/batch_inference.zip</b> - Zipped batch inference data.\n",
    "- <b>data/transform/full_query.csv</b> - Batch inference data. 25000 records.\n",
    "- <b>data/transform/short_query.csv</b> - Real-time inference data. 10 records.\n",
    "- <b>data/transform/realtime_inference.zip</b> - Zipped real-time inference data.\n",
    "\n",
    "## Training data\n",
    "The training data <b>training_data.csv</b> contains two comma separated columns \"Review\" and \"Label\". There is one record on each line. Note that there are 50,000 records. The NML script <b>train.nml</b> specifies a parameter \"validation_split\" with a value of 0.5, i.e. 50% of the data is to be withheld from training and just used for validation.  \n",
    "\n",
    "## Inference data\n",
    "The inference data <b>full_query.csv</b> only contains the \"Review\" column, and only contains the last 25,000 records present in <b>training_data.csv</b> This is then compressed in <b>batch_inference.zip</b>\n",
    "\n",
    "We have also created a smaller set for testing real-time inference: <b>short_query.csv</b>. This only contains the first 10 records of <b>full_query.csv</b>. This is then compressed into <b>realtime_inference.zip</b>\n",
    "\n",
    "The CSV files are not used directly, the script just leaves them for illustrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment and create a session\n",
    "\n",
    "We begin by setting up some environmental variables. In particular, our local data directories, the name and prefixes for the S3 bucket we will use, and the IAM role we will use. Please edit these to reflect the bucket/role you will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data directories\n",
    "TRAINING_WORKDIR = \"data/training/\"\n",
    "TRANSFORM_WORKDIR = \"data/transform/\"\n",
    "\n",
    "\n",
    "# AWS INFORMATION\n",
    "# S3 Bucket -- Replace this with the name of the S3 bucket you want to use (or leave it to use the default bucket name:)\n",
    "BUCKET = None\n",
    "\n",
    "# S3 Prefixes -- Replace these with \n",
    "COMMON_PREFIX = \"neopulse-sagemaker-demo-imbd\"\n",
    "TRAINING_PREFIX = COMMON_PREFIX + \"/training-data\"\n",
    "INFERENCE_PREFIX = COMMON_PREFIX + \"/inference-data\"\n",
    "\n",
    "# IAM Role -- Replace this with a valid IAM role\n",
    "IAM_ROLE = None\n",
    "\n",
    "# Algorithm ARN -- Replace this with the ARN of your NeoPulse® Algorithm Subscription\n",
    "ALGO_ARN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sm_sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3\n",
    "\n",
    "Now we use the session to upload the data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we upload the training data\n",
    "training_data = sm_sess.upload_data(TRAINING_WORKDIR, bucket=BUCKET, key_prefix=TRAINING_PREFIX)\n",
    "print (\"Training Data Location: \" + training_data)\n",
    "\n",
    "# Then the batch inference data\n",
    "batch_inference_data = sm_sess.upload_data(TRANSFORM_WORKDIR + 'batch_inference.zip', bucket=BUCKET, key_prefix=INFERENCE_PREFIX)\n",
    "print(\"Batch Inference Data Location: \" + batch_inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training Job using Algorithm ARN\n",
    "Now we create a training job by specifying the instance type and job name to the AlgorithmEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.algorithm import AlgorithmEstimator\n",
    "\n",
    "estimator = AlgorithmEstimator(sagemaker_session=sm_sess,\n",
    "                               algorithm_arn=ALGO_ARN,\n",
    "                               role=IAM_ROLE,\n",
    "                               train_instance_count=1,\n",
    "                               train_instance_type='ml.m4.xlarge',\n",
    "                               base_job_name='imdb-demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Job\n",
    "We use the estimator.fit function to train the model. You can check on the status of the training job at:\n",
    "https://us-east-2.console.aws.amazon.com/sagemaker/home?region=<b>your-region-here</b>#/jobs\n",
    "\n",
    "Look for a job starting with the <b>base_job_name</b> you specified in creating the estimator, above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Now run the training job using algorithm arn %s in region %s\" % (ALGO_ARN, sm_sess.boto_region_name))\n",
    "estimator.fit({'training': training_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Now run the training job using algorithm arn %s in region %s\" % (ALGO_ARN, sm_sess.boto_region_name))\n",
    "estimator.fit({'training': training_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform Job\n",
    "Now let's use the model built to run a batch inference job and verify it works.\n",
    "\n",
    "<b>NOTE:</b> Set the parameter <b>max_payload</b> to a value large enough to handle <b>batch_transform.zip</b>. Here we set it to 0, corresponding to unlimited payload size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(1, 'ml.m4.xlarge',max_payload=0)\n",
    "transformer.transform(batch_inference_data, content_type='application/zip',logs=True,wait=True)\n",
    "\n",
    "print(\"Batch Transform output saved to \" + transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Batch Transform Output in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "file_key = '{}/{}.out'.format(parsed_url.path[1:], \"batch_inference.zip\")\n",
    "print(bucket_name)\n",
    "print(file_key)\n",
    "s3_client = sm_sess.boto_session.client('s3')\n",
    "\n",
    "response = s3_client.get_object(Bucket = bucket_name, Key = file_key)\n",
    "response_text = response['Body'].read().decode('utf-8')\n",
    "print(response_text.split('\\n')[:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Inference Endpoint\n",
    "Finally, we demonstrate the creation of an endpoint for live inference using this neopulse algorithm generated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.predictor import zip_serializer\n",
    "predictor = estimator.deploy(1, 'ml.m4.xlarge', content_type=\"application/zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform real-time prediction\n",
    "We read the zip file as binary data and then use the predictor to get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRANSFORM_WORKDIR + \"realtime_inference.zip\",'rb') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(predictor.predict(data).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
